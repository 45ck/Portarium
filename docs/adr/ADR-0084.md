# ADR-0084: Multi-Tenant Storage Tier Automation

**Status**: Accepted
**Date**: 2026-02-22
**Bead**: bead-0392

## Context

Portarium serves tenants with very different scale and isolation requirements.
A single storage model cannot satisfy all of them cost-effectively:

| Tenant size   | Volume            | Isolation requirement                               |
| ------------- | ----------------- | --------------------------------------------------- |
| Small / trial | < 10 k rows/day   | Best-effort; shared DB acceptable                   |
| Growing       | 10 k–1 M rows/day | Dedicated schema; performance predictability        |
| Enterprise    | > 1 M rows/day    | Full DB isolation; compliance, BYOK, custom backups |

We previously operated a single shared PostgreSQL cluster. Scaling conflicts,
noisy-neighbour throughput degradation, and compliance requests (SOC 2 Type II,
GDPR data residency) require a structured multi-tier model.

## Decision

Two storage tiers, each with its own lifecycle automation:

### Tier 1 — Shared (schema-per-tenant)

- All shared-tier tenants live in one shared RDS instance (`portarium` DB).
- Each tenant gets its own PostgreSQL **schema** (`tenant_<id>`), owned by a
  dedicated role (`role_<id>`).
- A shared service account (`portarium_shared`) has default-privilege grants
  for DML operations inside each schema.
- Lifecycle (provision / deprovision) is handled in-process by
  `TenantStorageProvisioner` (TypeScript), called by the workspace use-cases.
- **Backup**: covered by the shared RDS automated backup window (daily,
  14-day retention in staging, 35-day in prod).

### Tier 2 — Dedicated (DB-per-tenant)

- Each dedicated-tier tenant gets its own **RDS instance** (PostgreSQL 16).
- Full lifecycle is managed by Terraform (`infra/terraform/modules/tenant-storage`).
- Credentials are stored in Vault KV v2 (`secret/tenants/<id>/db`) and
  delivered to pods via the Vault Secrets Operator.
- **Backup**: per-tenant AWS Backup plan (daily cron at 03:00 UTC) with
  configurable cross-region copy. Retention matches environment tier
  (14 days staging, 35 days prod).

### Tier promotion

Moving from shared → dedicated is a three-step migration:

1. Terraform provisions the new dedicated instance.
2. `pg_dump --schema=tenant_<id>` exports tenant data from the shared DB.
3. Data is restored into the dedicated instance.
4. Application switchover: update Vault secret path; zero-downtime via
   connection-string feature flag.
5. Terraform removes the shared schema entry (optional, after quiescing).

### Backup / restore per tier

| Tier      | Mechanism                                | Granularity               | RTO     | RPO   |
| --------- | ---------------------------------------- | ------------------------- | ------- | ----- |
| Shared    | RDS automated snapshots                  | Full DB (all tenants)     | ~30 min | 24 h  |
| Dedicated | AWS Backup (RDS) + optional cross-region | Per-tenant DB             | ~15 min | 24 h  |
| Dedicated | Point-in-time recovery (RDS PITR)        | Per-tenant DB, any minute | ~5 min  | 5 min |

Restore procedures are validated quarterly via DR drills (bead-0397).

## Consequences

**Positive**

- Shared tier is cost-efficient for small tenants (no per-tenant RDS cost).
- Dedicated tier provides full isolation, BYOK (customer-managed KMS keys),
  and compliance-grade backup granularity.
- Terraform module is reusable and idempotent.
- `TenantStorageProvisioner` is unit-testable without a real DB.

**Negative / Trade-offs**

- Dedicated tier has higher per-tenant cost (~$50–$200/month minimum for
  `db.t4g.medium`); only viable for enterprise-tier pricing.
- Schema-per-tenant on shared DB requires careful `search_path` hygiene in
  application queries to avoid cross-tenant data leakage. Connection pooler
  (PgBouncer) must be configured to reset `search_path` on each transaction.
- Tier migrations require a maintenance window (typically < 5 min).

## Alternatives Considered

- **Row-level security (RLS)** on a single schema: rejected — RLS policy
  overhead and `tenant_id` column proliferation conflict with existing domain
  model; policy audit surface is larger.
- **Separate RDS clusters per environment** only: insufficient — this doesn't
  address per-tenant isolation within an environment.
- **CockroachDB multi-region**: considered for a future global-tier; deferred
  as it requires re-testing all query patterns.

## Related

- ADR-0070: Hybrid orchestration + CloudEvents boundary
- ADR-0083: Environment model and artefact promotion pipeline
- bead-0397: DR drills and automated recovery validation
