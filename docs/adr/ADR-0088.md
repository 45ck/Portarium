# ADR-0088: DR Drills and Automated Recovery Validation

**Status**: Accepted
**Date**: 2026-02-22
**Bead**: bead-0397

## Context

Portarium's evidence store and workflow history must be recoverable after
infrastructure failures (RDS crash, region outage, accidental deletion).
Backup procedures exist (ADR-0084, ADR-0083) but have never been exercised
end-to-end under time pressure.

Untested backups are a liability, not an asset. We need:
1. Proof that backups can be restored to a usable state.
2. Proof that cluster infrastructure can be rebuilt from IaC.
3. Proof that evidence store replication is working and integrity is preserved.
4. A structured process that tracks DR readiness over time.

## Decision

Run automated DR drills **quarterly** (cron: 02:00 UTC on 1st Jan/Apr/Jul/Oct)
and on-demand via `workflow_dispatch`.

### Three scenarios

| Scenario | What is tested | Success criteria |
|---|---|---|
| `db-restore` | RDS snapshot → new instance → data integrity check | DR instance available in < 30 min; all integrity queries pass |
| `cluster-recreate` | `terraform apply` from scratch in DR region → K8s deploy → health probe | All deployments ready in < 15 min; `/health/ready` returns 200 |
| `evidence-replication` | Write test object → CRR lag ≤ 15 min → ETag match → object lock valid | Replication within SLA; ETag/size match; lock retention confirmed |

### Toolchain

- **GitHub Actions**: orchestration (`dr-drill.yml`), parallel scenario jobs.
- **AWS CLI + Terraform**: snapshot restore, cluster recreation.
- **Node.js scripts** (`scripts/dr/`): validation logic (headless, no test runner
  dependency in CI).
- **GitHub Issues**: drill tracking; issue auto-closes on success, stays open
  on partial failure for human follow-up.
- **Artefact retention**: drill result JSON files kept for 365 days.

### DR instance lifecycle

The DB restore scenario creates a `portarium-dr-<drill-id>` RDS instance
tagged `AutoDelete=true`. The job deletes it on both success and failure
(`if: always()`). The Terraform cluster is similarly destroyed at job end.
No DR resources persist beyond the drill window.

### RTO / RPO targets validated per drill

| Target | Metric | SLO |
|---|---|---|
| RTO (DB restore) | Time from snapshot-restore start to instance available | ≤ 30 min |
| RTO (cluster recreate) | Time from `terraform apply` start to health probe 200 | ≤ 15 min |
| RPO (evidence replication) | Replication lag of test object | ≤ 15 min |
| Data integrity | ETag match between source and replica | 100% |

If any target is missed, the drill issue stays open and triggers a
post-mortem within 5 business days.

### Evidence object-lock validation

The `validate-evidence-replication.mjs` script checks `GetObjectRetention`
on the source bucket to confirm that COMPLIANCE-mode object lock is applied.
Buckets without object lock (dev/staging) skip this check gracefully.

## Consequences

**Positive**
- Quarterly drills provide continuous evidence of recovery capability for
  SOC 2 Type II audits.
- Automated execution removes dependency on human memory and availability.
- DR issue tracking creates a permanent record of drill outcomes.
- Isolated DR resources (tagged `AutoDelete=true`) are safely cleaned up.

**Negative / Trade-offs**
- `cluster-recreate` drills incur ~$20–$50 in AWS costs per run (EKS cluster
  for ~30 minutes). Acceptable for quarterly frequency; skip on-demand.
- Terraform state isolation (separate key per drill) means the DR state is
  ephemeral; it is destroyed with the cluster and not kept for analysis.
- The `evidence-replication` scenario requires the replica bucket to exist and
  CRR to be configured; this must be provisioned before the first drill.

## Related

- ADR-0083: Environment model and artefact promotion
- ADR-0084: Multi-tenant storage tier automation (backup strategy)
- ADR-0085: SLO definitions
- bead-0395: CI OIDC federation (DR workflow uses same OIDC role)
